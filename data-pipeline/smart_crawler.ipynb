{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danfran/ai-agents/blob/main/data-pipeline/smart_crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langgraph langchain-tavily langgraph-checkpoint-sqlite langchain-anthropic langchain[google-genai] pydantic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "uUifnQ9AWG0X",
        "outputId": "42a8f3dd-0038-4c10-8669-dfd96b38cd5a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.6.6-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting langchain-tavily\n",
            "  Downloading langchain_tavily-0.2.11-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting langgraph-checkpoint-sqlite\n",
            "  Downloading langgraph_checkpoint_sqlite-2.0.11-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain-anthropic\n",
            "  Downloading langchain_anthropic-0.3.19-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.7)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.14)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.11.14 in /usr/local/lib/python3.12/dist-packages (from langchain-tavily) (3.12.15)\n",
            "Collecting aiosqlite>=0.20 (from langgraph-checkpoint-sqlite)\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting sqlite-vec>=0.1.6 (from langgraph-checkpoint-sqlite)\n",
            "  Downloading sqlite_vec-0.1.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux1_x86_64.whl.metadata (198 bytes)\n",
            "Collecting anthropic<1,>=0.64.0 (from langchain-anthropic)\n",
            "  Downloading anthropic-0.64.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting langchain-google-genai (from langchain[google-genai])\n",
            "  Downloading langchain_google_genai-2.1.9-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.20.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1,>=0.64.0->langchain-anthropic) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1,>=0.64.0->langchain-anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1,>=0.64.0->langchain-anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1,>=0.64.0->langchain-anthropic) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic<1,>=0.64.0->langchain-anthropic) (1.3.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai->langchain[google-genai])\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai->langchain[google-genai])\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (5.29.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic<1,>=0.64.0->langchain-anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic<1,>=0.64.0->langchain-anthropic) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (4.9.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (0.6.1)\n",
            "Downloading langgraph-0.6.6-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_tavily-0.2.11-py3-none-any.whl (26 kB)\n",
            "Downloading langgraph_checkpoint_sqlite-2.0.11-py3-none-any.whl (31 kB)\n",
            "Downloading langchain_anthropic-0.3.19-py3-none-any.whl (31 kB)\n",
            "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Downloading anthropic-0.64.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.2/297.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.2-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlite_vec-0.1.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux1_x86_64.whl (151 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.1.9-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sqlite-vec, filetype, ormsgpack, aiosqlite, langgraph-sdk, anthropic, langgraph-checkpoint, langchain-anthropic, google-ai-generativelanguage, langgraph-prebuilt, langgraph-checkpoint-sqlite, langchain-google-genai, langgraph, langchain-tavily\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiosqlite-0.21.0 anthropic-0.64.0 filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain-anthropic-0.3.19 langchain-google-genai-2.1.9 langchain-tavily-0.2.11 langgraph-0.6.6 langgraph-checkpoint-2.1.1 langgraph-checkpoint-sqlite-2.0.11 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.2 ormsgpack-1.10.0 sqlite-vec-0.1.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "078d6d8c38ee407999e8ebeb213afeb0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import re\n",
        "import urllib.parse\n",
        "from datetime import datetime\n",
        "import sqlite3\n",
        "\n",
        "import pandas as pd # For convenience in database interaction verification\n",
        "from bs4 import BeautifulSoup # Used directly in the tool, ensure it's imported\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from typing import TypedDict, List, Dict, Union, Any, Annotated\n",
        "\n",
        "# --- 0. Environment Setup ---\n",
        "# Ensure GOOGLE_API_KEY is set. If running in Google Colab, use userdata.get().\n",
        "# If running locally, set it as an environment variable (e.g., in .env file).\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "    # os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY') # Not strictly needed for this example, but common for agents\n",
        "except ImportError:\n",
        "    if not os.getenv('GOOGLE_API_KEY'):\n",
        "        print(\"GOOGLE_API_KEY not found in environment variables. Please set it.\")\n",
        "\n",
        "\n",
        "# --- 1. Define LangGraph State ---\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of the agent's process.\n",
        "\n",
        "    Attributes:\n",
        "        messages (List[Union[HumanMessage, AIMessage, ToolMessage]]): Conversation history including agent's thoughts and tool outputs.\n",
        "        initial_webpage_url (str): The starting URL provided by the user.\n",
        "        link_description (str): User's description of the content of the file to find on the page.\n",
        "        webpage_content (str): The text content of the browsed webpage.\n",
        "        extracted_links (List[Dict[str, str]]): List of extracted link text and hrefs.\n",
        "        chosen_download_url (str | None): The URL identified by the agent for download.\n",
        "        downloaded_filepath (str | None): The local path where the file was saved.\n",
        "        status (str): Current status of the agent's process (e.g., \"initiated\", \"browsed\", \"link_identified\", \"downloaded\", \"stored\", \"failed\").\n",
        "        errors (List[str]): List of error messages encountered.\n",
        "    \"\"\"\n",
        "    messages: Annotated[list, add_messages]\n",
        "    initial_webpage_url: str\n",
        "    link_description: str\n",
        "    webpage_content: str\n",
        "    extracted_links: List[Dict[str, str]]\n",
        "    chosen_download_url: str | None\n",
        "    downloaded_filepath: str | None\n",
        "    status: str\n",
        "    errors: List[str]\n",
        "\n",
        "# --- 2. Define Custom Tools ---\n",
        "\n",
        "@tool\n",
        "def fetch_webpage_content_and_links(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Fetches the content of a webpage and extracts all visible links,\n",
        "    returning a formatted string for the LLM to process.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Fetching content from: {url}\")\n",
        "        response = requests.get(url, timeout=15)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract main text content (a simplified approach)\n",
        "        page_text = soup.get_text(separator=' ', strip=True)\n",
        "        # Limit to first N characters to avoid overwhelming LLM with too much content\n",
        "        content_summary = page_text[:4000] + (\"...\" if len(page_text) > 4000 else \"\") # Increased content for better reasoning\n",
        "\n",
        "        # Extract links\n",
        "        links = []\n",
        "        for i, a_tag in enumerate(soup.find_all('a', href=True)):\n",
        "            href = a_tag['href']\n",
        "            link_text = a_tag.get_text(strip=True)\n",
        "            # Make sure href is absolute if it's relative\n",
        "            if href.startswith('/'):\n",
        "                href = urllib.parse.urljoin(url, href)\n",
        "            elif not href.startswith(('http://', 'https://')):\n",
        "                continue # Skip non-http/https relative links\n",
        "\n",
        "            # Filter out very short or empty link text\n",
        "            if link_text and len(link_text) > 2:\n",
        "                links.append({\"id\": i, \"text\": link_text, \"href\": href})\n",
        "\n",
        "        # Convert links list to a string for LLM consumption\n",
        "        # Include an ID for each link to potentially refer back to\n",
        "        links_str = \"\\n\".join([f\"Link ID: {link['id']}, Text: '{link['text']}', URL: '{link['href']}'\" for link in links[:100]]) # Increased limit\n",
        "        if len(links) > 100:\n",
        "            links_str += f\"\\n... (and {len(links) - 100} more links)\"\n",
        "\n",
        "        return (f\"Webpage content summary (first 4000 chars):\\n{content_summary}\\n\\n\"\n",
        "                f\"Extracted links ({len(links)} total, first 100 shown):\\n{links_str}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error fetching webpage content from {url}: {e}\"\n",
        "    except Exception as e:\n",
        "        return f\"An unexpected error occurred while parsing webpage: {e}\"\n",
        "\n",
        "@tool\n",
        "def download_file(url: str, filename: str | None = None) -> str:\n",
        "    \"\"\"\n",
        "    Downloads a file from a given URL and saves it locally.\n",
        "    Attempts to infer filename from URL or Content-Disposition header.\n",
        "    Args:\n",
        "        url (str): The URL of the file to download. This URL MUST be the exact, full URL\n",
        "                   identified by the agent from the webpage.\n",
        "        filename (str | None): Optional. The name to save the file as.\n",
        "                                If None, attempts to infer from URL or headers.\n",
        "    Returns:\n",
        "        str: A message indicating success or failure of the download.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Attempting to download file from: {url}\")\n",
        "        response = requests.get(url, stream=True, timeout=30)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "        # Determine the filename to save the file\n",
        "        if filename is None:\n",
        "            # Try to get filename from Content-Disposition header\n",
        "            if \"Content-Disposition\" in response.headers:\n",
        "                cd = response.headers['Content-Disposition']\n",
        "                fname_match = re.findall(r'filename\\*?=([^;]+)', cd)\n",
        "                if fname_match:\n",
        "                    filename_header = fname_match[0].strip().replace('\"', '')\n",
        "                    if filename_header.lower().startswith(\"utf-8''\"):\n",
        "                        filename = urllib.parse.unquote(filename_header[7:])\n",
        "                    else:\n",
        "                        filename = filename_header\n",
        "\n",
        "            # If still no filename, try from URL path\n",
        "            if filename is None:\n",
        "                parsed_url = urllib.parse.urlparse(url)\n",
        "                path_segments = parsed_url.path.split('/')\n",
        "                potential_filename = path_segments[-1] if path_segments[-1] else \"downloaded_file\"\n",
        "                filename = potential_filename.split('?')[0].split('#')[0]\n",
        "                if not filename:\n",
        "                    filename = \"downloaded_file\"\n",
        "\n",
        "        # Add a generic extension if none found/inferred and Content-Type isn't clear\n",
        "        if '.' not in filename and 'Content-Type' in response.headers:\n",
        "            content_type = response.headers['Content-Type'].split(';')[0]\n",
        "            if 'image/jpeg' in content_type: filename += '.jpg'\n",
        "            elif 'image/png' in content_type: filename += '.png'\n",
        "            elif 'application/pdf' in content_type: filename += '.pdf'\n",
        "            elif 'text/csv' in content_type: filename += '.csv'\n",
        "            elif 'text/plain' in content_type: filename += '.txt'\n",
        "            # Add more types as needed\n",
        "\n",
        "        if '.' not in filename: # Final fallback if no extension\n",
        "            filename += '.bin'\n",
        "\n",
        "        print(f\"Saving file as: {filename}\")\n",
        "        with open(filename, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        return f\"Successfully downloaded file to '{filename}'.\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error downloading file from {url}: {e}\"\n",
        "    except Exception as e:\n",
        "        return f\"An unexpected error occurred during download: {e}\"\n",
        "\n",
        "@tool\n",
        "def save_download_metadata(url_discovered: str, filename: str) -> str:\n",
        "    \"\"\"\n",
        "    Stores metadata about a downloaded file (timestamp, URL, filename) into an SQLite database.\n",
        "    \"\"\"\n",
        "    db_path = 'download_logs.db'\n",
        "    conn = None\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS downloads (\n",
        "                timestamp TEXT,\n",
        "                url_discovered TEXT,\n",
        "                filename TEXT\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        current_timestamp = datetime.now().isoformat()\n",
        "        cursor.execute(\"INSERT INTO downloads (timestamp, url_discovered, filename) VALUES (?, ?, ?)\",\n",
        "                       (current_timestamp, url_discovered, filename))\n",
        "        conn.commit()\n",
        "        return f\"Metadata saved to '{db_path}': URL='{url_discovered}', File='{filename}'.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error saving metadata to DB: {e}\"\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "\n",
        "# --- 3. Initialize LLM and Agent Executor ---\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\") # Use a model capable of tool calling\n",
        "\n",
        "# All tools available to the agent\n",
        "tools = [fetch_webpage_content_and_links, download_file, save_download_metadata]\n",
        "\n",
        "# Define the agent's prompt\n",
        "# The prompt is critical for guiding the LLM's reasoning to identify the correct URL.\n",
        "agent_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"You are a highly intelligent and meticulous web crawling agent.\n",
        "            Your primary task is to find and download a specific file from a given webpage\n",
        "            based on a detailed description of the file's content. After downloading, you\n",
        "            must log the action to a database.\n",
        "\n",
        "            **Here's your step-by-step reasoning process:**\n",
        "\n",
        "            1.  **Understand the Request:** Carefully read the user's initial request.\n",
        "                You will be given an `initial_webpage_url` to visit and a `link_description`\n",
        "                that describes the *content* of the file you need to find.\n",
        "            2.  **Browse the Page:** Use the `fetch_webpage_content_and_links` tool\n",
        "                with the `initial_webpage_url`. Pay close attention to the returned\n",
        "                `Webpage content summary` and the `Extracted links` (their text and URLs).\n",
        "            3.  **Identify the Target URL:** Based on the `link_description` provided by the user,\n",
        "                analyze the `Extracted links` from the webpage. Your goal is to find the\n",
        "                **exact URL** (`href`) that most accurately matches the description of the file's content.\n",
        "                Consider both the link's visible text and the overall webpage content summary\n",
        "                to determine relevance. If a link's text directly contains keywords from the description,\n",
        "                that's a strong candidate. Look for context clues.\n",
        "                **Crucially, you must select the *full, absolute URL* of the file.**\n",
        "            4.  **Download the File:** Once you have confidently identified the correct URL,\n",
        "                call the `download_file` tool. Pass the identified URL to its `url` argument.\n",
        "                You can optionally suggest a `filename` for the downloaded file that\n",
        "                reflects its content (e.g., \"HM_Land_Registry_Price_Paid_Data.csv\").\n",
        "            5.  **Log the Download:** After receiving confirmation from the `download_file` tool\n",
        "                that the download was successful, use the `save_download_metadata` tool.\n",
        "                Provide the `url_discovered` (the one you just downloaded) and the `filename`\n",
        "                that the `download_file` tool reported.\n",
        "            6.  **Summarize and Confirm:** Finally, provide a concise and clear summary\n",
        "                of your actions and the outcome to the user, confirming whether the file was found, downloaded,\n",
        "                and logged successfully. If you couldn't find the link or encountered any errors,\n",
        "                explain what happened.\n",
        "            \"\"\",\n",
        "        ),\n",
        "        # CORRECTED: Explicitly define placeholders for chat_history, input, and agent_scratchpad\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create the LangChain tool-calling agent\n",
        "agent = create_tool_calling_agent(llm, tools, agent_prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "\n",
        "# --- 4. Define LangGraph Nodes ---\n",
        "# Each node is a step in our graph.\n",
        "\n",
        "def call_agent_executor(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Calls the LangChain AgentExecutor to run the main logic.\n",
        "    The agent will use its tools based on the current state.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Node: Calling Agent Executor ---\")\n",
        "    new_state = state.copy()\n",
        "    try:\n",
        "        # Pass the current 'input' and 'chat_history' explicitly to the agent executor.\n",
        "        # The agent_executor internally manages 'agent_scratchpad' based on its turns.\n",
        "        result = agent_executor.invoke({\n",
        "            \"input\": new_state['messages'][-1].content, # Last message is the current user input\n",
        "            \"chat_history\": new_state['messages'][:-1] # All messages before the last one form chat history\n",
        "        })\n",
        "\n",
        "        # Capture the entire message history from the agent_executor's run\n",
        "        # For simplicity, we'll append the last AI message from the agent's output.\n",
        "        if result and \"output\" in result:\n",
        "             # The result['output'] is the agent's final answer string.\n",
        "             # We add it as an AIMessage to the state's messages.\n",
        "             new_state['messages'].append(AIMessage(content=result['output']))\n",
        "             new_state['status'] = \"agent_completed\"\n",
        "             print(\"Agent Executor completed its turn.\")\n",
        "        else:\n",
        "             new_state['errors'].append(\"Agent Executor returned no output.\")\n",
        "             new_state['status'] = \"failed\"\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error during agent execution: {e}\"\n",
        "        print(error_msg)\n",
        "        new_state['errors'].append(error_msg)\n",
        "        new_state['status'] = \"failed\"\n",
        "\n",
        "    return new_state\n",
        "\n",
        "\n",
        "# --- 5. Define LangGraph Workflow ---\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Add the single main node which runs the agent executor\n",
        "workflow.add_node(\"agent_node\", call_agent_executor)\n",
        "\n",
        "# Set the entry point to the agent node\n",
        "workflow.set_entry_point(\"agent_node\")\n",
        "\n",
        "# The agent node decides when it's done, which for this example means the workflow ends.\n",
        "# In a more complex scenario, the agent might decide to loop or transition to another\n",
        "# specific non-agent node for post-processing.\n",
        "workflow.add_edge(\"agent_node\", END)\n",
        "\n",
        "\n",
        "# Compile the workflow\n",
        "app = workflow.compile()\n",
        "\n",
        "# --- 6. Example Usage ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # For a real public agent, you'd use a live URL.\n",
        "    # For this local example, we'll use a publicly accessible HTML page\n",
        "    # from the UK government that *might* contain a link relevant to the description.\n",
        "    # Note: Finding an exact match for the complex description on a random page\n",
        "    # is challenging without very precise prompt engineering and a real dataset.\n",
        "    # This URL is illustrative. A real scenario might require more specific scraping.\n",
        "\n",
        "    # Example URL for HM Land Registry data (a common public data source in the UK)\n",
        "    # This is a sample URL, the agent's ability to find the exact file depends heavily\n",
        "    # on the real content of this page and the LLM's reasoning capabilities.\n",
        "    initial_page_url = \"https://www.gov.uk/government/statistical-data-sets/price-paid-data-downloads\"\n",
        "\n",
        "    # User's highly descriptive request for the file content\n",
        "    link_description = \"the file that includes standard and additional price paid data transactions received at HM Land Registry from 1 January 1995 to the most current monthly data.\"\n",
        "\n",
        "    # User's initial input to kick off the process\n",
        "    user_query = (\n",
        "        f\"I need to find a specific dataset. \"\n",
        "        f\"Please go to the webpage: {initial_page_url}. \"\n",
        "        f\"On that page, identify and download {link_description}. \"\n",
        "        \"After downloading, please log the download information into the database.\"\n",
        "    )\n",
        "\n",
        "    initial_state: AgentState = {\n",
        "        \"messages\": [HumanMessage(content=user_query)],\n",
        "        \"initial_webpage_url\": initial_page_url,\n",
        "        \"link_description\": link_description,\n",
        "        \"webpage_content\": \"\", # This will be populated by the tool\n",
        "        \"extracted_links\": [], # This will be populated by the tool\n",
        "        \"chosen_download_url\": None,\n",
        "        \"downloaded_filepath\": None,\n",
        "        \"status\": \"initiated\",\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    print(\"\\n--- Starting Web ETL Agent Process ---\")\n",
        "    final_state = app.invoke(initial_state)\n",
        "\n",
        "    print(\"\\n--- Web ETL Agent Process Finished ---\")\n",
        "    print(f\"Final Status: {final_state['status']}\")\n",
        "    if final_state['errors']:\n",
        "        print(\"Errors encountered:\")\n",
        "        for error in final_state['errors']:\n",
        "            print(f\"- {error}\")\n",
        "\n",
        "    # Verify database contents\n",
        "    db_path = 'download_logs.db'\n",
        "    if os.path.exists(db_path):\n",
        "        print(f\"\\n--- Verifying Database Contents from '{db_path}' ---\")\n",
        "        conn = None\n",
        "        try:\n",
        "            conn = sqlite3.connect(db_path)\n",
        "            df_logs = pd.read_sql_query(\"SELECT * FROM downloads\", conn)\n",
        "            print(df_logs)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading database: {e}\")\n",
        "        finally:\n",
        "            if conn:\n",
        "                conn.close()\n",
        "\n",
        "    # The dummy HTML file is not needed if using a live public URL\n",
        "    # if os.path.exists(\"dummy_webpage.html\"):\n",
        "    #     os.remove(\"dummy_webpage.html\")\n",
        "    #     print(f\"\\nCleaned up 'dummy_webpage.html'.\")\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Web ETL Agent Process ---\n",
            "\n",
            "--- Node: Calling Agent Executor ---\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `fetch_webpage_content_and_links` with `{'url': 'https://www.gov.uk/government/statistical-data-sets/price-paid-data-downloads'}`\n",
            "\n",
            "\n",
            "\u001b[0mFetching content from: https://www.gov.uk/government/statistical-data-sets/price-paid-data-downloads\n",
            "\u001b[36;1m\u001b[1;3mWebpage content summary (first 4000 chars):\n",
            "Price Paid Data - GOV.UK Cookies on GOV.UK We use some essential cookies to make this website work. We’d like to set additional cookies to understand how you use GOV.UK, remember your settings and improve government services. We also use cookies set by other sites to help us deliver content from their services. You have accepted additional cookies. You can change your cookie settings at any time. You have rejected additional cookies. You can change your cookie settings at any time. Accept additional cookies Reject additional cookies View cookies Hide this message Skip to main content GOV.UK Navigation menu Menu Menu Search GOV.UK × Search GOV.UK Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search Search GOV.UK Search Home Housing, local and community Housing and communities Statistical data set Price Paid Data Download the Price Paid Data (PPD) in text or CSV format and access our linked data. From: HM Land Registry Published 24 July 2014 Last updated 28 July 2025\n",
            "          — See all updates Contents Using or publishing our Price Paid Data June 2025 data (current month) Single file Yearly file Accuracy of the data Our Price Paid Data includes information on all property sales in England and Wales that are sold for value and are lodged with us for registration. Get up to date with the permitted use of our Price Paid Data: check what to consider when using or publishing our Price Paid Data Using or publishing our Price Paid Data If you use or publish our Price Paid Data, you must add the following attribution statement: Contains HM Land Registry data © Crown copyright and database right 2021. This data is licensed under the Open Government Licence v3.0. Price Paid Data is released under the Open Government Licence ( OGL ) . You need to make sure you understand the terms of the OGL before using the data. Under the OGL , HM Land Registry permits you to use the Price Paid Data for commercial or non-commercial purposes. However, OGL does not cover the use of third party rights, which we are not authorised to license. Price Paid Data contains address data processed against Ordnance Survey’s AddressBase Premium product, which incorporates Royal Mail’s PAF® database (Address Data). Royal Mail and Ordnance Survey permit your use of Address Data in the Price Paid Data: for personal and/or non-commercial use to display for the purpose of providing residential property price information services If you want to use the Address Data in any other way, you must contact Royal Mail. Email address.management@royalmail.com . Address data The following fields comprise the address data included in Price Paid Data: Postcode PAON Primary Addressable Object Name (typically the house number or name) SAON Secondary Addressable Object Name – if there is a sub-building, for example, the building is divided into flats, there will be a SAON Street Locality Town/City District County June 2025 data (current month) The June 2025 release includes: the first release of data for June 2025 (transactions received from the first to the last day of the month) updates to earlier data releases Standard Price Paid Data (SPPD) and Additional Price Paid Data (APPD) transactions As we will be adding to the June data in future releases, we...\n",
            "\n",
            "Extracted links (118 total, first 100 shown):\n",
            "Link ID: 0, Text: 'change your cookie settings', URL: 'https://www.gov.uk/help/cookies'\n",
            "Link ID: 1, Text: 'change your cookie settings', URL: 'https://www.gov.uk/help/cookies'\n",
            "Link ID: 2, Text: 'View cookies', URL: 'https://www.gov.uk/help/cookies'\n",
            "Link ID: 4, Text: 'GOV.UK', URL: 'https://www.gov.uk'\n",
            "Link ID: 5, Text: 'Menu', URL: 'https://www.gov.uk/browse'\n",
            "Link ID: 6, Text: 'Search GOV.UK', URL: 'https://www.gov.uk/search'\n",
            "Link ID: 7, Text: 'Benefits', URL: 'https://www.gov.uk/browse/benefits'\n",
            "Link ID: 8, Text: 'Births, death, marriages and care', URL: 'https://www.gov.uk/browse/births-deaths-marriages'\n",
            "Link ID: 9, Text: 'Business and self-employed', URL: 'https://www.gov.uk/browse/business'\n",
            "Link ID: 10, Text: 'Childcare and parenting', URL: 'https://www.gov.uk/browse/childcare-parenting'\n",
            "Link ID: 11, Text: 'Citizenship and living in the UK', URL: 'https://www.gov.uk/browse/citizenship'\n",
            "Link ID: 12, Text: 'Crime, justice and the law', URL: 'https://www.gov.uk/browse/justice'\n",
            "Link ID: 13, Text: 'Disabled people', URL: 'https://www.gov.uk/browse/disabilities'\n",
            "Link ID: 14, Text: 'Driving and transport', URL: 'https://www.gov.uk/browse/driving'\n",
            "Link ID: 15, Text: 'Education and learning', URL: 'https://www.gov.uk/browse/education'\n",
            "Link ID: 16, Text: 'Employing people', URL: 'https://www.gov.uk/browse/employing-people'\n",
            "Link ID: 17, Text: 'Environment and countryside', URL: 'https://www.gov.uk/browse/environment-countryside'\n",
            "Link ID: 18, Text: 'Housing and local services', URL: 'https://www.gov.uk/browse/housing-local-services'\n",
            "Link ID: 19, Text: 'Money and tax', URL: 'https://www.gov.uk/browse/tax'\n",
            "Link ID: 20, Text: 'Passports, travel and living abroad', URL: 'https://www.gov.uk/browse/abroad'\n",
            "Link ID: 21, Text: 'Visas and immigration', URL: 'https://www.gov.uk/browse/visas-immigration'\n",
            "Link ID: 22, Text: 'Working, jobs and pensions', URL: 'https://www.gov.uk/browse/working'\n",
            "Link ID: 23, Text: 'Departments', URL: 'https://www.gov.uk/government/organisations'\n",
            "Link ID: 24, Text: 'News', URL: 'https://www.gov.uk/search/news-and-communications'\n",
            "Link ID: 25, Text: 'Guidance and regulation', URL: 'https://www.gov.uk/search/guidance-and-regulation'\n",
            "Link ID: 26, Text: 'Research and statistics', URL: 'https://www.gov.uk/search/research-and-statistics'\n",
            "Link ID: 27, Text: 'Policy papers and consultations', URL: 'https://www.gov.uk/search/policy-papers-and-consultations'\n",
            "Link ID: 28, Text: 'Transparency', URL: 'https://www.gov.uk/search/transparency-and-freedom-of-information-releases'\n",
            "Link ID: 29, Text: 'Home', URL: 'https://www.gov.uk/'\n",
            "Link ID: 30, Text: 'Housing, local and community', URL: 'https://www.gov.uk/housing-local-and-community'\n",
            "Link ID: 31, Text: 'Housing and communities', URL: 'https://www.gov.uk/housing-local-and-community/housing-and-communities'\n",
            "Link ID: 32, Text: 'HM Land Registry', URL: 'https://www.gov.uk/government/organisations/land-registry'\n",
            "Link ID: 40, Text: 'Open Government Licence (OGL)', URL: 'http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/'\n",
            "Link ID: 43, Text: 'conditions', URL: 'https://www.gov.uk/government/statistical-data-sets/price-paid-data-downloads#using-or-publishing-our-price-paid-data'\n",
            "Link ID: 44, Text: 'current month as a CSV file', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-monthly-update-new-version.csv'\n",
            "Link ID: 45, Text: 'current month as a text file', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-monthly-update.txt'\n",
            "Link ID: 46, Text: 'conditions', URL: 'https://www.gov.uk/government/statistical-data-sets/price-paid-data-downloads#using-or-publishing-our-price-paid-data'\n",
            "Link ID: 47, Text: 'the complete Price Paid Transaction Data as a text file', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-complete.txt'\n",
            "Link ID: 48, Text: 'the complete Price Paid Transaction Data as a CSV file', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-complete.csv'\n",
            "Link ID: 49, Text: 'conditions', URL: 'https://www.gov.uk/government/statistical-data-sets/price-paid-data-downloads#using-or-publishing-our-price-paid-data'\n",
            "Link ID: 50, Text: 'the yearly data as a text file', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2025.txt'\n",
            "Link ID: 51, Text: 'the yearly data as a CSV file (complete)', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2025.csv'\n",
            "Link ID: 52, Text: 'the yearly data as a text file', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2024.txt'\n",
            "Link ID: 53, Text: 'the yearly data as a CSV file (complete)', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2024.csv'\n",
            "Link ID: 54, Text: 'the yearly data as a text file', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2023.txt'\n",
            "Link ID: 55, Text: 'the yearly data as a CSV file (complete)', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2023.csv'\n",
            "Link ID: 56, Text: 'part 1 of the yearly data as a CSV file (part 1)', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2023-part1.csv'\n",
            "Link ID: 57, Text: 'part 2 of the yearly data as a CSV file (part 2)', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2023-part2.csv'\n",
            "Link ID: 58, Text: 'the yearly data as a text file', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2022.txt'\n",
            "Link ID: 59, Text: 'the yearly data as a CSV file (complete)', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2022.csv'\n",
            "Link ID: 60, Text: 'part 1 of the yearly data as a CSV file (part 1)', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2022-part1.csv'\n",
            "Link ID: 61, Text: 'part 2 of the yearly data as a CSV file (part 2)', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2022-part2.csv'\n",
            "Link ID: 62, Text: 'the complete yearly data as a text file', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2021.txt'\n",
            "Link ID: 63, Text: 'the complete yearly data as a CSV file (complete)', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2021.csv'\n",
            "Link ID: 64, Text: 'part 1 of the yearly data as a CSV file (part 1)', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2021-part1.csv'\n",
            "Link ID: 65, Text: 'part 2 of the yearly data as a CSV file (part 2)', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2021-part2.csv'\n",
            "Link ID: 66, Text: 'the complete yearly data as a text file', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2020.txt'\n",
            "Link ID: 67, Text: 'the complete yearly data as a CSV file (complete)', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2020.csv'\n",
            "Link ID: 68, Text: 'part 1 of the yearly data as a CSV file (part 1)', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2020-part1.csv'\n",
            "Link ID: 69, Text: 'part 2 of the yearly data as a CSV file (part 2)', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2020-part2.csv'\n",
            "Link ID: 70, Text: 'the complete yearly data as a text file', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2019.txt'\n",
            "Link ID: 71, Text: 'the complete yearly data as a CSV file (complete)', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2019.csv'\n",
            "Link ID: 72, Text: 'part 1 of the yearly data as a CSV file (part 1)', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2019-part1.csv'\n",
            "Link ID: 73, Text: 'part 2 of the yearly data as a CSV file (part 2)', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2019-part2.csv'\n",
            "Link ID: 74, Text: 'the complete yearly data as a text file', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2018.txt'\n",
            "Link ID: 75, Text: 'the complete yearly data as a CSV file (complete)', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2018.csv'\n",
            "Link ID: 76, Text: 'part 1 of the yearly data as a CSV file (part 1)', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2018-part1.csv'\n",
            "Link ID: 77, Text: 'part 2 of the yearly data as a CSV file (part 2)', URL: 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2018-part2.csv'\n",
            "Link ID: 78, Text: 'previous year’s data', URL: 'https://data.gov.uk/dataset/314f77b3-e702-4545-8bcb-9ef8262ea0fd/archived-price-paid-data-1995-to-2017'\n",
            "Link ID: 79, Text: 'linked data', URL: 'https://landregistry.data.gov.uk/#linked-data'\n",
            "Link ID: 80, Text: 'Price Paid Data report builder', URL: 'http://landregistry.data.gov.uk/app/ppd'\n",
            "Link ID: 81, Text: 'Data excluded from Price Paid Data', URL: 'https://www.gov.uk/about-the-price-paid-data#data-excluded-from-price-paid-data'\n",
            "Link ID: 82, Text: 'accuracy of the data', URL: 'https://www.gov.uk/government/publications/hm-land-registry-data/public-data#accuracy-of-the-data'\n",
            "Link ID: 83, Text: 'change a sales record form', URL: 'https://customerhelp.landregistry.gov.uk/changeasalesrecordwebform'\n",
            "Link ID: 86, Text: 'HM Land Registry: Transaction Data', URL: 'https://www.gov.uk/guidance/hm-land-registry-transaction-data'\n",
            "Link ID: 87, Text: 'HM Land Registry: Price Paid Data', URL: 'https://www.gov.uk/government/collections/price-paid-data'\n",
            "Link ID: 88, Text: 'Price Paid Data: Standard reports', URL: 'https://www.gov.uk/guidance/price-paid-data-standard-reports'\n",
            "Link ID: 89, Text: 'How to access HM Land Registry Price Paid Data', URL: 'https://www.gov.uk/guidance/about-the-price-paid-data'\n",
            "Link ID: 90, Text: 'HM Land Registry data', URL: 'https://www.gov.uk/government/publications/hm-land-registry-data'\n",
            "Link ID: 91, Text: 'HM Land Registry: Price Paid Data', URL: 'https://www.gov.uk/government/collections/price-paid-data'\n",
            "Link ID: 92, Text: 'Housing and communities', URL: 'https://www.gov.uk/housing-local-and-community/housing-and-communities'\n",
            "Link ID: 93, Text: 'Land Registration Data', URL: 'https://www.gov.uk/housing-local-and-community/land-registration-data'\n",
            "Link ID: 94, Text: 'Maybe', URL: 'https://www.gov.uk/contact/govuk'\n",
            "Link ID: 95, Text: 'Please fill in this survey (opens in a new taband requires JavaScript)', URL: 'https://www.smartsurvey.co.uk/s/gov-uk-banner/?c=no-js'\n",
            "Link ID: 96, Text: 'Benefits', URL: 'https://www.gov.uk/browse/benefits'\n",
            "Link ID: 97, Text: 'Births, death, marriages and care', URL: 'https://www.gov.uk/browse/births-deaths-marriages'\n",
            "Link ID: 98, Text: 'Business and self-employed', URL: 'https://www.gov.uk/browse/business'\n",
            "Link ID: 99, Text: 'Childcare and parenting', URL: 'https://www.gov.uk/browse/childcare-parenting'\n",
            "Link ID: 100, Text: 'Citizenship and living in the UK', URL: 'https://www.gov.uk/browse/citizenship'\n",
            "Link ID: 101, Text: 'Crime, justice and the law', URL: 'https://www.gov.uk/browse/justice'\n",
            "Link ID: 102, Text: 'Disabled people', URL: 'https://www.gov.uk/browse/disabilities'\n",
            "Link ID: 103, Text: 'Driving and transport', URL: 'https://www.gov.uk/browse/driving'\n",
            "Link ID: 104, Text: 'Education and learning', URL: 'https://www.gov.uk/browse/education'\n",
            "Link ID: 105, Text: 'Employing people', URL: 'https://www.gov.uk/browse/employing-people'\n",
            "Link ID: 106, Text: 'Environment and countryside', URL: 'https://www.gov.uk/browse/environment-countryside'\n",
            "Link ID: 107, Text: 'Housing and local services', URL: 'https://www.gov.uk/browse/housing-local-services'\n",
            "Link ID: 108, Text: 'Money and tax', URL: 'https://www.gov.uk/browse/tax'\n",
            "Link ID: 109, Text: 'Passports, travel and living abroad', URL: 'https://www.gov.uk/browse/abroad'\n",
            "Link ID: 110, Text: 'Visas and immigration', URL: 'https://www.gov.uk/browse/visas-immigration'\n",
            "Link ID: 111, Text: 'Working, jobs and pensions', URL: 'https://www.gov.uk/browse/working'\n",
            "... (and 18 more links)\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `download_file` with `{'filename': 'HM_Land_Registry_Price_Paid_Data_Complete.csv', 'url': 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-complete.csv'}`\n",
            "\n",
            "\n",
            "\u001b[0mAttempting to download file from: http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-complete.csv\n",
            "Saving file as: HM_Land_Registry_Price_Paid_Data_Complete.csv\n",
            "\u001b[33;1m\u001b[1;3mSuccessfully downloaded file to 'HM_Land_Registry_Price_Paid_Data_Complete.csv'.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `save_download_metadata` with `{'url_discovered': 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-complete.csv', 'filename': 'HM_Land_Registry_Price_Paid_Data_Complete.csv'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3mMetadata saved to 'download_logs.db': URL='http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-complete.csv', File='HM_Land_Registry_Price_Paid_Data_Complete.csv'.\u001b[0m\u001b[32;1m\u001b[1;3mI have successfully downloaded the file containing standard and additional price paid data transactions received at HM Land Registry from 1 January 1995 to the most current monthly data. The file was saved as `HM_Land_Registry_Price_Paid_Data_Complete.csv`. I have also logged this download to the database.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Agent Executor completed its turn.\n",
            "\n",
            "--- Web ETL Agent Process Finished ---\n",
            "Final Status: agent_completed\n",
            "\n",
            "--- Verifying Database Contents from 'download_logs.db' ---\n",
            "                    timestamp  \\\n",
            "0  2025-08-20T16:48:20.331603   \n",
            "\n",
            "                                      url_discovered  \\\n",
            "0  http://prod.publicdata.landregistry.gov.uk.s3-...   \n",
            "\n",
            "                                        filename  \n",
            "0  HM_Land_Registry_Price_Paid_Data_Complete.csv  \n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zof58AqVVpZ",
        "outputId": "13d24489-b02d-42a8-ecb3-14c8baea5f8a"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}